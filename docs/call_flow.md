================================================================================
  GCC_GraphControl ì‹¤í–‰ íë¦„ (í•¨ìˆ˜ í˜¸ì¶œ ìˆœì„œ)
  ================================================================================

  1. í”„ë¡œê·¸ë¨ ì‹œì‘
     â”œâ”€ graphcontrol.py:main()
     â”‚
     â”œâ”€ [1-1] ë°ì´í„°ì…‹ ë¡œë“œ
     â”‚  â””â”€ NodeDataset(config.dataset, n_seeds=config.seeds)
     â”‚     â””â”€ datasets/node_dataset.py
     â”‚
     â”œâ”€ [1-2] Condition ìƒì„± (ì „ì²´ ê·¸ë˜í”„, í•œ ë²ˆë§Œ)
     â”‚  â””â”€ obtain_attributes(dataset_obj.data, use_adj=False, threshold=config.threshold)
     â”‚     â””â”€ utils/transforms.py:obtain_attributes()
     â”‚        â”œâ”€ similarity(data.x, data.x)  # X @ X.T
     â”‚        â”‚  â””â”€ utils/normalize.py:similarity()
     â”‚        â”œâ”€ torch.where(tmp > threshold, 1.0, 0.0)  # ì´ì§„í™”
     â”‚        â”œâ”€ get_laplacian_matrix(tmp)
     â”‚        â”‚  â””â”€ utils/normalize.py:get_laplacian_matrix()
     â”‚        â””â”€ torch.linalg.eigh(tmp)  # ê³ ìœ ê°’ ë¶„í•´
     â”‚        â””â”€ return V[:, :32]  # x_sim [N, 32]
     â”‚
     â”œâ”€ [1-3] Train/Test ë°ì´í„° ì¤€ë¹„
     â”‚  â””â”€ preprocess(config, dataset_obj, device)
     â”‚     â””â”€ graphcontrol.py:preprocess()
     â”‚        â”œâ”€ process_attributes(dataset_obj.data, ...)  # Laplacian PE
     â”‚        â”‚  â””â”€ utils/transforms.py:process_attributes()
     â”‚        â”‚     â””â”€ obtain_attributes(data, use_adj=True, ...)
     â”‚        â”‚        â””â”€ (ìœ„ì™€ ë™ì¼, í•˜ì§€ë§Œ adjacency ê¸°ë°˜)
     â”‚        â”‚
     â”‚        â””â”€ collect_subgraphs(dataset_obj, ...)  # Random walk subgraphs
     â”‚           â””â”€ utils/sampling.py:collect_subgraphs()
     â”‚              â”œâ”€ random_walk_with_restart(...)
     â”‚              â””â”€ return train_loader, test_loader
     â”‚
     â””â”€ [1-4] ê° ì‹œë“œë³„ë¡œ ì‹¤í—˜ ë°˜ë³µ
        â””â”€ for i, seed in enumerate(config.seeds):


  2. ëª¨ë¸ ìƒì„± (ê° ì‹œë“œë§ˆë‹¤)
     â””â”€ load_model(num_node_features, dataset_obj.num_classes, config)
        â””â”€ models/model_manager.py:load_model()
           â”œâ”€ torch.load('checkpoint/gcc.pth')  # Pretrained weights
           â”‚
           â”œâ”€ GCC_GraphControl.__init__(**kwargs)
           â”‚  â””â”€ models/gcc_graphcontrol.py:GCC_GraphControl.__init__()
           â”‚     â”œâ”€ self.encoder = GCC(**kwargs)
           â”‚     â”‚  â””â”€ models/gcc.py:GCC.__init__()
           â”‚     â”‚     â”œâ”€ GraphEncoder(...) ì´ˆê¸°í™”
           â”‚     â”‚     â”œâ”€ 5ê°œ GIN layers ìƒì„±
           â”‚     â”‚     â””â”€ graph_readout (JK-style)
           â”‚     â”‚
           â”‚     â”œâ”€ self.trainable_copy = copy.deepcopy(self.encoder)
           â”‚     â”‚
           â”‚     â”œâ”€ self.cond_proj = nn.Linear(32, 128)  # SHARED
           â”‚     â”œâ”€ self.cond_input_adapter = nn.Linear(128, 32)
           â”‚     â”œâ”€ self.zero_layers = nn.ModuleList([...])  # 5ê°œ
           â”‚     â””â”€ self.linear_classifier = nn.Linear(128, num_classes)
           â”‚
           â”œâ”€ model.encoder.load_state_dict(params)  # Frozen
           â””â”€ model.trainable_copy.load_state_dict(params)  # Trainable


  3. Fine-tuning
     â””â”€ finetune(config, model, train_loader, device, x_sim, test_loader)
        â””â”€ graphcontrol.py:finetune()
           â”‚
           â”œâ”€ [3-1] Freeze encoder
           â”‚  â””â”€ for k, v in model.named_parameters():
           â”‚     if 'encoder' in k: v.requires_grad = False
           â”‚
           â”œâ”€ [3-2] Reset classifier
           â”‚  â””â”€ model.reset_classifier()
           â”‚     â””â”€ models/gcc_graphcontrol.py:reset_classifier()
           â”‚
           â”œâ”€ [3-3] Training loop
           â”‚  â””â”€ for epoch in range(config.epochs):
           â”‚     â”‚
           â”‚     â”œâ”€ [3-3-1] Training
           â”‚     â”‚  â””â”€ for data in train_loader:
           â”‚     â”‚     â”œâ”€ Sign flip augmentation
           â”‚     â”‚     â”‚  â””â”€ x = data.x * sign_flip
           â”‚     â”‚     â”‚
           â”‚     â”‚     â”œâ”€ Extract condition for batch
           â”‚     â”‚     â”‚  â””â”€ x_sim = full_x_sim[data.original_idx]
           â”‚     â”‚     â”‚     # [N_full, 32] â†’ [N_batch, 32]
           â”‚     â”‚     â”‚
           â”‚     â”‚     â”œâ”€ Forward pass
           â”‚     â”‚     â”‚  â””â”€ model.forward_subgraph(x, x_sim, ...)
           â”‚     â”‚     â”‚     â”‚
           â”‚     â”‚     â”‚     â””â”€ models/gcc_graphcontrol.py:forward_subgraph()
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â”œâ”€ [Step 1] Condition processing (ONCE)
           â”‚     â”‚     â”‚        â”‚  â”œâ”€ cond_hidden = self.cond_proj(x_sim)
           â”‚     â”‚     â”‚        â”‚  â”‚  # [N, 32] â†’ [N, 128]
           â”‚     â”‚     â”‚        â”‚  â””â”€ cond_first_layer = self.cond_input_adapter(cond_hidden)
           â”‚     â”‚     â”‚        â”‚     # [N, 128] â†’ [N, 32]
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â”œâ”€ [Step 2] Prepare initial features
           â”‚     â”‚     â”‚        â”‚  â”œâ”€ self.encoder.prepare_node_features(x, ...)
           â”‚     â”‚     â”‚        â”‚  â”‚  â””â”€ models/gcc.py:prepare_node_features()
           â”‚     â”‚     â”‚        â”‚  â”‚     â””â”€ embedding = self.node_embedding(x)
           â”‚     â”‚     â”‚        â”‚  â”‚        # [N, 32] â†’ [N, 32]
           â”‚     â”‚     â”‚        â”‚  â”‚
           â”‚     â”‚     â”‚        â”‚  â””â”€ self.trainable_copy.prepare_node_features(x, ...)
           â”‚     â”‚     â”‚        â”‚     # ë™ì¼
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â”œâ”€ [Step 3] Layer-wise forward (5 layers)
           â”‚     â”‚     â”‚        â”‚  â””â”€ for layer_idx in range(5):
           â”‚     â”‚     â”‚        â”‚     â”‚
           â”‚     â”‚     â”‚        â”‚     â”œâ”€ Frozen branch
           â”‚     â”‚     â”‚        â”‚     â”‚  â””â”€ h_frozen = layer_frozen(h_frozen, edge_index)
           â”‚     â”‚     â”‚        â”‚     â”‚     â””â”€ models/gcc.py:GINConv.forward()
           â”‚     â”‚     â”‚        â”‚     â”‚        â”œâ”€ MLP(x)
           â”‚     â”‚     â”‚        â”‚     â”‚        â””â”€ aggregate neighbors
           â”‚     â”‚     â”‚        â”‚     â”‚
           â”‚     â”‚     â”‚        â”‚     â”œâ”€ Trainable branch
           â”‚     â”‚     â”‚        â”‚     â”‚  â”œâ”€ if layer_idx == 0:
           â”‚     â”‚     â”‚        â”‚     â”‚  â”‚  â””â”€ ctrl_input = h_ctrl + cond_first_layer
           â”‚     â”‚     â”‚        â”‚     â”‚  â”œâ”€ else:
           â”‚     â”‚     â”‚        â”‚     â”‚  â”‚  â””â”€ ctrl_input = h_ctrl + cond_hidden
           â”‚     â”‚     â”‚        â”‚     â”‚  â”‚
           â”‚     â”‚     â”‚        â”‚     â”‚  â””â”€ h_ctrl = layer_ctrl(ctrl_input, edge_index)
           â”‚     â”‚     â”‚        â”‚     â”‚     â””â”€ models/gcc.py:GINConv.forward()
           â”‚     â”‚     â”‚        â”‚     â”‚
           â”‚     â”‚     â”‚        â”‚     â””â”€ Inject into frozen
           â”‚     â”‚     â”‚        â”‚        â””â”€ h_frozen = h_frozen + zero_layer(h_ctrl)
           â”‚     â”‚     â”‚        â”‚           â””â”€ nn.Linear (zero initialized)
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â”œâ”€ [Step 4] Graph readout
           â”‚     â”‚     â”‚        â”‚  â””â”€ out, _ = self.encoder.gnn.graph_readout(hidden_states, batch)
           â”‚     â”‚     â”‚        â”‚     â””â”€ models/gcc.py:GraphEncoder.graph_readout()
           â”‚     â”‚     â”‚        â”‚        â”œâ”€ Concatenate all layer outputs
           â”‚     â”‚     â”‚        â”‚        â””â”€ Global mean pooling per graph
           â”‚     â”‚     â”‚        â”‚           # [N, 128] â†’ [B, 128]
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â”œâ”€ [Step 5] Normalization
           â”‚     â”‚     â”‚        â”‚  â””â”€ out = F.normalize(out, p=2, dim=-1)
           â”‚     â”‚     â”‚        â”‚
           â”‚     â”‚     â”‚        â””â”€ [Step 6] Classification
           â”‚     â”‚     â”‚           â””â”€ x = self.linear_classifier(out)
           â”‚     â”‚     â”‚              # [B, 128] â†’ [B, num_classes]
           â”‚     â”‚     â”‚
           â”‚     â”‚     â”œâ”€ Loss computation
           â”‚     â”‚     â”‚  â””â”€ loss = criterion(preds, data.y)
           â”‚     â”‚     â”‚     â””â”€ nn.CrossEntropyLoss
           â”‚     â”‚     â”‚
           â”‚     â”‚     â””â”€ Backward & Update
           â”‚     â”‚        â”œâ”€ loss.backward()
           â”‚     â”‚        â””â”€ optimizer.step()
           â”‚     â”‚
           â”‚     â””â”€ [3-3-2] Evaluation (every 3 epochs)
           â”‚        â””â”€ if epoch % 3 == 0:
           â”‚           â””â”€ eval_subgraph(config, model, test_loader, device, x_sim)


  4. Evaluation
     â””â”€ eval_subgraph(config, model, test_loader, device, full_x_sim)
        â””â”€ graphcontrol.py:eval_subgraph()
           â””â”€ for batch in test_loader:
              â”œâ”€ x_sim = full_x_sim[batch.original_idx]
              â”œâ”€ preds = model.forward_subgraph(batch.x, x_sim, ...)
              â”‚  â””â”€ (ìœ„ì˜ forward_subgraphì™€ ë™ì¼)
              â””â”€ correct += (preds == batch.y).sum()


  5. ìµœì¢… ê²°ê³¼
     â””â”€ for i, seed in enumerate(config.seeds):
        â”œâ”€ best_acc = finetune(...)
        â”œâ”€ acc_list.append(best_acc)
        â””â”€ print(f'Seed: {seed}, Accuracy: {best_acc}')
     
     â””â”€ print(f"# final_acc: {mean}Â±{std}")


  ================================================================================
  ì½”ë“œ ì½ê¸° ìˆœì„œ ì¶”ì²œ
  ================================================================================

  ğŸ“– **ì´ˆë³´ììš© ìˆœì„œ (ê°œë… ì´í•´ ìš°ì„ ):**

  1. graphcontrol.py:main()
     â””â”€ ì „ì²´ íë¦„ íŒŒì•…

  2. utils/transforms.py:obtain_attributes()
     â””â”€ Conditionì´ ë¬´ì—‡ì¸ì§€ ì´í•´

  3. models/gcc.py:GCC
     â””â”€ Pretrained encoder êµ¬ì¡° ì´í•´

  4. models/gcc_graphcontrol.py:GCC_GraphControl.__init__()
     â””â”€ ControlNet ì•„í‚¤í…ì²˜ ì´í•´

  5. models/gcc_graphcontrol.py:forward_subgraph()
     â””â”€ í•µì‹¬ ë¡œì§ (frozen + trainable + injection)

  6. graphcontrol.py:finetune()
     â””â”€ í•™ìŠµ ê³¼ì •


  ğŸ“– **ë””ë²„ê¹…ìš© ìˆœì„œ (ì‹¤í–‰ íë¦„ ì¶”ì ):**

  1. graphcontrol.py:main() [line 112]
     â†“
  2. utils/transforms.py:obtain_attributes() [line 58]
     â†“
  3. graphcontrol.py:preprocess() [line 16]
     â†“
  4. models/model_manager.py:load_model() [line 6]
     â†“
  5. models/gcc_graphcontrol.py:GCC_GraphControl.__init__() [line 12]
     â†“
  6. graphcontrol.py:finetune() [line 37]
     â†“
  7. models/gcc_graphcontrol.py:forward_subgraph() [line 74]
     â†“
  8. models/gcc.py:prepare_node_features() [line X]
     â†“
  9. models/gcc.py:GINConv.forward() [line X]
     â†“
  10. models/gcc.py:graph_readout() [line X]


  ğŸ“– **í•µì‹¬ë§Œ ë¹ ë¥´ê²Œ:**

  1. models/gcc_graphcontrol.py:forward_subgraph() [line 74]
     â””â”€ ì—¬ê¸°ê°€ ëª¨ë“  í•µì‹¬!

  2. utils/transforms.py:obtain_attributes() [line 58]
     â””â”€ Condition ìƒì„±

  3. graphcontrol.py:finetune() [line 37]
     â””â”€ í•™ìŠµ ë£¨í”„


  ================================================================================

  í•µì‹¬ í¬ì¸íŠ¸:

  1. Conditionì€ í•œ ë²ˆë§Œ ê³„ì‚°: main()ì—ì„œ ì „ì²´ ê·¸ë˜í”„ë¡œ x_sim ê³„ì‚° â†’ ëª¨ë“  epoch/batchì—ì„œ ì¬ì‚¬ìš©
  2. Shared projection: cond_projëŠ” ëª¨ë“  ë ˆì´ì–´ê°€ ê³µìœ  (í•œ ë²ˆë§Œ ê³„ì‚°)
  3. Layer-wise injection: zero_layers[0~4]ë¥¼ í†µí•´ ê° ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ zero convolution
  4. Frozen vs Trainable: EncoderëŠ” frozen, trainable_copy + zero_layers + classifierë§Œ í•™ìŠµ
